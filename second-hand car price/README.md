
# 零基础入门数据挖掘 -二手车交易价格预测

2020.3.10-4.11天池与开源AI学习组织Datawhale合作举办的数据竞赛，详情见（包含赛题说明与数据下载）：https://tianchi.aliyun.com/competition/entrance/231784/information

主要分为赛题理解，数据探索，特征工程，建模调参与模型融合这五部分。


## 1、赛题理解

主要是对赛题进行分析，对数据进行先验知识的理解。
我看竞赛5群有个队伍的赛题理解挖的蛮深的，贴一下他们的连接：https://shimo.im/docs/RhQKcjD8qtw9XQYt/read.
关于数据理解方面，主要包括字段含义（明确特征处理，匿名特征处理等等）、数据量（电脑配置）、评测标准、结果提交（注意格式要求），我们也可以从已给的数据当中构造属性，eg：此次竞赛的用车时间大约等于广告发布时间-汽车注册时间，即usedate=creatdate-regdate。
在看完赛题与数据之后，我们根据自己已有的经验，在心里给自己一个baseline方案。随后，再在baseline方案的基础上去不断修改自己的版本（注意版本迭代的保存，方便自己返回之前的版本），直至采用的评价准则达到比较好的效果。


## 2、数据探索

我把它分为如下几个小点：

### 2.1 整体数据把握：
包括读入数据之后，首先利用.shape查看数据维度，然后利用.head()与.tail()查看数据的前五行和最后五行了解其列数以及每列的具体value.

### 2.2 数据统计信息：
包括利用.info()查看列索引名称，每列有多少缺失值以及每列的数据类型（float,object等等);利用.describe()查看每列个数，平均值，最大值，最小值，3/4分位数，标准差等等值。

### 2.3 异常值的处理：
利用.isnull().sum()可查看数据每一列的缺失个数情况，我们也可以利用柱形图plot.bar()以及.matrix(data.sample(250/1k))等方法对缺失值进行可视化，至于如何处理异常值，我们可以使用.dropna()进行过滤，也可以使用.fillna(x)将其补全为x；对特殊的列进行处理，例如对于类型为object的列，我们可以利用.value.counts()查看此列的各值统计的种类与个数情况等等，对于不合理的value我们可以对其进行替换，eg:利用.replace('-',np.nan,inplace=True)将'-'替换为缺失值NAN等，再比如对于数据分布严重倾斜的列，其统计是无意义的，因此我们可以选择将其删除，eg，此次比赛的“seler”与“offertype”

### 2.4 查看预测分布：
对于我们整个模型需要预测的数据price，我们要通过训练数据的直接输出和.value_counts()对它有一个整体的把握。对其（1）总体分布概况，符合正态，对数正态还是无界约翰逊分布，（若进行回归分析，则需要保证分布符合正态，有时可通过对数据求对数log以满足前提假设）；（2)2) 查看偏度和峰度skewness and kurtosis；

### 2.5 特征分类判别：
对于特征，我们一般将其分为类别特征或者数字特征，可以通过dtype是number or object来简单地辨别，但真实场景中此种方法肯定是不行的。因为有时提供的数据早已对本来应该是类别的特征数字化，所以我们就需要通过已有的先验知识以及常识来对特征进行分类，分为numeric_features与categorical_features。并对于categorical_features特征通过.nunique()来查看其分布情况，包括种类与个数等等。
  
  [2.5.1] 数字特征分析
   对于数字特征，我们可以通过
   （1）相关性分析.corr()再构造热力图sns.heatmap()很明显地看出各个数字特征之间地相关性是高度相关还是基本不相关，且关注除price以外地特征对price的具体影响。
   （2）查看几个数字特征的偏度和峰值情况。
   （3）通过pd.melt()对每个数字特征本身的分布进行可视化sns.FacetGrid()再总体统计.map(sns.distplot()).
   （4）多变量互相回归关系可视化，可视化每一个特征对于输出price的影响，若某些分布图高度相似，说明这些特征高度相关，可能只要一个就可以表达出对price同样的效果。
   
   [2.5.2] 类别特征分析
   （1）unique分布，通过.unique()查看其是否是稀疏的，删除特别稀疏的特征，因为对于结果的预测没有实际的意义。
   （2）类别特征箱形图可视化，通过箱型图查看是否有异常值，也可以看出不同数据分布的对比。
   （3）类别特征的小提琴图可视化（每个变量与price的关系图可视化），箱型图进阶版，最大的不同是不仅可以看出是否有异常值，还能够看出在每个value上面的个数fenbuqk，数据集中或者稀疏分布在那个区域等等。
   （4）类别特征的柱形图可视化
   
   ### 2.6 用pandas_profiling生成数据报告：
   这是pandas一个非常强大的功能，其不仅对数据总体进行了统计，还对每个变量的情况（是否稀疏，基数是否大等等）进行了统计，个人觉得几乎包含了前面所有的手动数据探索，而且warnings部分还能对你特征工程的构造有很多启发，例如删除太稀疏的特征，高相关的特征多取一等等情况，真是一大利器，疯狂打call。不过话说回来，生成这个html可视化版本在我的小笔记本上面花费了很多时间，奈何我不会用服务器~
   
   
   
   ## 3、特征工程
   
   ### 3.1 异常值处理：
   定义了一个利用箱型图来删除大于up-bound或者小于low-bound的自定义函数outliers_proc，可直接将数据data，列名col_name以及尺度scale=3（默认为三）（即异常值的范围为大于0.75+3*（0.75-0.25)，小于同理可得）传递给函数形参进行操作。 
   
   ### 3.2 特征构造：
   利用现有的特征构造出对于预测标签有用的特征。例如（1）利用regdate-creatdate得到汽车使用时间，而我们根据先验可知，预测价格一般与汽车使用时间呈现反比例关系。（2）从邮编中提取城市信息，提取邮编regionCode的前两位作为城市信息。（3）对train 数据计算统计量，也就是其特征与price的关系？（4）对数据进行分桶以离散化，也就是属性合并功能，从而使特征对异常值具有鲁棒性，同时对于LR等等模型具有较好的泛化性等等增强模型的表达能力(LightGBM 在改进 XGBoost 时就增加了数据分桶)。（5）删除不需要的数据，或者之前利用数据构造过新特征的数据，'creatDate', 'regDate', 'regionCode'。（6）对特征进行可视化plot.hist()以了解其分布，对于LR模型，其对于分布有一定的要求（数据不能是稀疏的），因此可对于某些特征例如power进行取对数log再进行归一化。再对于所有特征进行one hot编码，即可得到适用于LR模型的特征。
   
   ### 3.3 特征筛选:
   （1）过滤式：利用corr计算相关性，从而过滤相关性很高的特征，因为其对于price的作用是类似的。（2）包裹式：直接把最终将要使用的学习器的性能作为特征子集的评价准则，常见方法有 LVM（Las Vegas Wrapper）这里是利用mlxtend包。（3）嵌入式：下一章介绍，Lasso 回归和决策树可以完成嵌入式特征选择（大部分情况下都是用嵌入式做特征筛选）
   
   特征工程其实是和模型结合在一起的，这就是为什么要为 LR NN 做分桶和特征归一化的原因，而对于特征的处理效果和特征重要性等往往要通过模型来验证。
   总的来说，特征工程是一个入门简单，但想精通非常难的一件事。
   需要注意的是，特征工程其实是为了模型服务的。所以我们必须根据模型所需要的数据来适当地转换数据，例如这里TREE与LR模型所需要地数据就是不一样地。第（6）步之前的步骤所完成的特征就可用于决策树，然而对于逻辑回归来讲却是还不够的，需要归一再one hot编码。（具体原因还没有完全搞清楚，我先记着）
   
   另外提一句，所有EDA的目的都是为了后续更好地进行特征工程，即EDA是为了特征工程而存在地=的。而工程又是为了模型而进行的，因此这是一连贯的动作。
  
